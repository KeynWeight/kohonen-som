{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cbee55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1: Basic SOM with visualization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training SOM: 100%|██████████| 200/200 [00:03<00:00, 63.74it/s, QE=0.0026, σ=0.010, α=0.0010]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Plotting quantization error...\n",
      "QE plot saved to qe_plot.png\n",
      "Visualizing SOM weights...\n",
      "SOM visualization saved to som_weights.png\n",
      "Model saved to trained_som.pkl\n",
      "Model saved!\n",
      "Total epochs trained: 200\n",
      "Total samples seen: 20000\n",
      "\n",
      "Example 2: Incremental training\n",
      "Model loaded!\n",
      "Additional training complete. Total epochs: 300\n",
      "QE plot saved to qe_plot_continued.png\n",
      "\n",
      "Example 3: Comparing initialization strategies\n",
      "random: QE = 0.0217\n",
      "SOM visualization saved to som_random.png\n",
      "pca: QE = 0.0127\n",
      "SOM visualization saved to som_pca.png\n",
      "linear: QE = 0.0124\n",
      "SOM visualization saved to som_linear.png\n",
      "\n",
      "Example 4: Comparing distance metrics\n",
      "euclidean: QE = 0.0217\n",
      "manhattan: QE = 0.0649\n",
      "cosine: QE = 0.0001\n",
      "\n",
      "Example 5: Backward compatible function\n",
      "Weights shape: (10, 10, 3)\n",
      "\n",
      "Example 6: Testing with different feature counts\n",
      "n_features=1: Training successful\n",
      "SOM visualization saved to som_1features.png\n",
      "n_features=2: Training successful\n",
      "SOM visualization saved to som_2features.png\n",
      "n_features=3: Training successful\n",
      "SOM visualization saved to som_3features.png\n",
      "n_features=5: Training successful\n",
      "SOM visualization saved to som_5features.png\n",
      "\n",
      "Final visualization saved as som_final.png\n",
      "\n",
      "All visualizations have been saved as PNG files:\n",
      "- qe_plot.png: Quantization error over epochs\n",
      "- som_weights.png: Main SOM weight visualization\n",
      "- qe_plot_continued.png: QE after continued training\n",
      "- som_random.png, som_pca.png, som_linear.png: Different init strategies\n",
      "- som_1features.png through som_5features.png: Different feature counts\n",
      "- som_final.png: Final result\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree, BallTree  # Issue #4: For efficient BMU search\n",
    "from tqdm import tqdm  # Issue #10: For progress monitoring\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Optional, Callable, List, Dict, Any, Tuple, Union\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "# ARCHITECTURE FIX #1: Enum classes for configuration management\n",
    "class Topology(Enum):\n",
    "    \"\"\"Issue #15: Support different grid topologies\"\"\"\n",
    "\n",
    "    RECTANGULAR = \"rectangular\"\n",
    "    HEXAGONAL = \"hexagonal\"\n",
    "    TOROIDAL = \"toroidal\"\n",
    "\n",
    "\n",
    "class DecaySchedule(Enum):\n",
    "    \"\"\"Issue #18 & #19: Different decay schedules for learning rate\"\"\"\n",
    "\n",
    "    EXPONENTIAL = \"exponential\"\n",
    "    LINEAR = \"linear\"\n",
    "    INVERSE = \"inverse\"\n",
    "    COSINE = \"cosine\"\n",
    "    STEP = \"step\"\n",
    "\n",
    "\n",
    "class LearningMode(Enum):\n",
    "    \"\"\"Issue #21: Different learning modes\"\"\"\n",
    "\n",
    "    ONLINE = \"online\"\n",
    "    BATCH = \"batch\"\n",
    "    MINI_BATCH = \"mini_batch\"\n",
    "\n",
    "\n",
    "class DistanceMetric(Enum):\n",
    "    \"\"\"ARCHITECTURE FIX #5: Support multiple distance metrics\"\"\"\n",
    "\n",
    "    EUCLIDEAN = \"euclidean\"\n",
    "    MANHATTAN = \"manhattan\"\n",
    "    COSINE = \"cosine\"\n",
    "    CHEBYSHEV = \"chebyshev\"\n",
    "\n",
    "\n",
    "class InitStrategy(Enum):\n",
    "    \"\"\"ARCHITECTURE FIX #6: Different initialization strategies\"\"\"\n",
    "\n",
    "    RANDOM = \"random\"\n",
    "    PCA = \"pca\"\n",
    "    SAMPLE = \"sample\"\n",
    "    LINEAR = \"linear\"\n",
    "\n",
    "\n",
    "# ARCHITECTURE FIX #2: Configuration management with dataclass\n",
    "@dataclass\n",
    "class SOMConfig:\n",
    "    \"\"\"Centralized configuration management for SOM parameters\"\"\"\n",
    "\n",
    "    # Basic parameters\n",
    "    width: int\n",
    "    height: int\n",
    "    n_features: int = 3\n",
    "    n_iterations: int = 1000\n",
    "\n",
    "    # Training parameters\n",
    "    learning_mode: LearningMode = LearningMode.ONLINE\n",
    "    batch_size: int = 32\n",
    "\n",
    "    # Decay schedules (Issues #18, #19, #20)\n",
    "    sigma_decay: DecaySchedule = DecaySchedule.EXPONENTIAL\n",
    "    alpha_decay: DecaySchedule = DecaySchedule.EXPONENTIAL\n",
    "    initial_sigma: Optional[float] = None  # Auto-calculated if None\n",
    "    initial_alpha: float = 0.1\n",
    "    min_sigma: float = 0.01\n",
    "    min_alpha: float = 0.001\n",
    "    warmup_steps: int = 0\n",
    "\n",
    "    # Topology and distance (Issue #15, ARCHITECTURE FIX #5)\n",
    "    topology: Topology = Topology.RECTANGULAR\n",
    "    distance_metric: DistanceMetric = DistanceMetric.EUCLIDEAN\n",
    "\n",
    "    # Optimization parameters\n",
    "    cutoff_factor: float = 3.0  # Issues #13, #14\n",
    "    momentum: float = 0.0  # Issue #24\n",
    "    dtype: np.dtype = np.float32  # OPTIMIZATION\n",
    "\n",
    "    # Convergence parameters (Issues #16, #17)\n",
    "    early_stopping: bool = True\n",
    "    convergence_tolerance: float = 1e-4\n",
    "    patience: int = 10\n",
    "    convergence_check_interval: int = 10\n",
    "\n",
    "    # Performance parameters\n",
    "    kdtree_rebuild_interval: int = 25\n",
    "\n",
    "    # Initialization (ARCHITECTURE FIX #6)\n",
    "    init_strategy: InitStrategy = InitStrategy.RANDOM\n",
    "\n",
    "    # Persistence (ARCHITECTURE FIX #3, #4)\n",
    "    checkpoint_interval: Optional[int] = None\n",
    "    checkpoint_dir: str = \"checkpoints\"\n",
    "\n",
    "    # Bounds (Issue #23)\n",
    "    weight_bounds: Tuple[float, float] = (0.0, 1.0)\n",
    "\n",
    "    # Reproducibility (Issue #11)\n",
    "    seed: Optional[int] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"Auto-calculate initial sigma if not provided\"\"\"\n",
    "        if self.initial_sigma is None:\n",
    "            self.initial_sigma = max(self.width, self.height) / 2\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert config to dictionary for serialization\"\"\"\n",
    "        config_dict = asdict(self)\n",
    "        # Convert enums to strings\n",
    "        for key, value in config_dict.items():\n",
    "            if isinstance(value, Enum):\n",
    "                config_dict[key] = value.value\n",
    "        return config_dict\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, config_dict: Dict) -> \"SOMConfig\":\n",
    "        \"\"\"Create config from dictionary\"\"\"\n",
    "        # Convert string back to enums\n",
    "        enum_fields = {\n",
    "            \"topology\": Topology,\n",
    "            \"sigma_decay\": DecaySchedule,\n",
    "            \"alpha_decay\": DecaySchedule,\n",
    "            \"learning_mode\": LearningMode,\n",
    "            \"distance_metric\": DistanceMetric,\n",
    "            \"init_strategy\": InitStrategy,\n",
    "        }\n",
    "        for field_name, enum_class in enum_fields.items():\n",
    "            if field_name in config_dict and isinstance(config_dict[field_name], str):\n",
    "                config_dict[field_name] = enum_class(config_dict[field_name])\n",
    "        return cls(**config_dict)\n",
    "\n",
    "\n",
    "# ARCHITECTURE FIX #9: Callback system for monitoring and intervention\n",
    "class Callback(ABC):\n",
    "    \"\"\"Abstract base class for callbacks\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_epoch_begin(self, epoch: int, som: \"SOM\") -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_epoch_end(self, epoch: int, som: \"SOM\", metrics: Dict) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_training_begin(self, som: \"SOM\") -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def on_training_end(self, som: \"SOM\") -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "class CheckpointCallback(Callback):\n",
    "    \"\"\"ARCHITECTURE FIX #4: Callback for checkpointing during training\"\"\"\n",
    "\n",
    "    def __init__(self, checkpoint_dir: str, interval: int = 100):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.interval = interval\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, som: \"SOM\") -> None:\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, som: \"SOM\", metrics: Dict) -> None:\n",
    "        if epoch % self.interval == 0:\n",
    "            checkpoint_path = os.path.join(\n",
    "                self.checkpoint_dir, f\"checkpoint_epoch_{epoch}.pkl\"\n",
    "            )\n",
    "            try:\n",
    "                som.save(checkpoint_path)\n",
    "                if som.verbose:\n",
    "                    print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "            except (IOError, OSError) as e:\n",
    "                if som.verbose:\n",
    "                    print(f\"Warning: Failed to save checkpoint: {e}\")\n",
    "\n",
    "    def on_training_begin(self, som: \"SOM\") -> None:\n",
    "        pass\n",
    "\n",
    "    def on_training_end(self, som: \"SOM\") -> None:\n",
    "        final_path = os.path.join(self.checkpoint_dir, \"final_model.pkl\")\n",
    "        try:\n",
    "            som.save(final_path)\n",
    "        except (IOError, OSError) as e:\n",
    "            if som.verbose:\n",
    "                print(f\"Warning: Failed to save final model: {e}\")\n",
    "\n",
    "\n",
    "class EarlyStoppingCallback(Callback):\n",
    "    \"\"\"ARCHITECTURE FIX #9: Callback for custom early stopping logic\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, monitor: str = \"qe\", patience: int = 10, min_delta: float = 1e-4\n",
    "    ):\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_value = float(\"inf\")\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_epoch_begin(self, epoch: int, som: \"SOM\") -> None:\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch: int, som: \"SOM\", metrics: Dict) -> None:\n",
    "        current_value = metrics.get(self.monitor, float(\"inf\"))\n",
    "        if current_value < self.best_value - self.min_delta:\n",
    "            self.best_value = current_value\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            if self.wait >= self.patience:\n",
    "                som.stop_training = True\n",
    "                if som.verbose:\n",
    "                    print(f\"Early stopping triggered at epoch {epoch}\")\n",
    "\n",
    "    def on_training_begin(self, som: \"SOM\") -> None:\n",
    "        self.best_value = float(\"inf\")\n",
    "        self.wait = 0\n",
    "\n",
    "    def on_training_end(self, som: \"SOM\") -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "# ARCHITECTURE FIX #5: Distance metric functions\n",
    "class DistanceCalculator:\n",
    "    \"\"\"Calculate distances using different metrics\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def euclidean(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "        return np.linalg.norm(a - b, axis=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def manhattan(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "        return np.sum(np.abs(a - b), axis=-1)\n",
    "\n",
    "    @staticmethod\n",
    "    def cosine(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "        # Cosine distance = 1 - cosine similarity\n",
    "        dot_product = np.sum(a * b, axis=-1)\n",
    "        norm_a = np.linalg.norm(a, axis=-1)\n",
    "        norm_b = np.linalg.norm(b, axis=-1)\n",
    "        similarity = dot_product / (norm_a * norm_b + 1e-8)\n",
    "        return 1 - similarity\n",
    "\n",
    "    @staticmethod\n",
    "    def chebyshev(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "        return np.max(np.abs(a - b), axis=-1)\n",
    "\n",
    "\n",
    "# ARCHITECTURE FIX #1: Main SOM class with proper OOP design\n",
    "class SOM:\n",
    "    \"\"\"\n",
    "    Self-Organizing Map with comprehensive architecture improvements\n",
    "\n",
    "    Fixes all 23 original issues plus 9 architecture improvements\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: SOMConfig, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize SOM with configuration\n",
    "\n",
    "        Args:\n",
    "            config: SOMConfig object with all parameters\n",
    "            verbose: Whether to print training progress\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Issue #11: Set random seed for reproducibility\n",
    "        if config.seed is not None:\n",
    "            np.random.seed(config.seed)\n",
    "\n",
    "        # Initialize core attributes\n",
    "        self.n_neurons = config.width * config.height\n",
    "        self.weights_flat = None\n",
    "        self.tree = None  # Changed from kdtree to tree (can be KDTree or BallTree)\n",
    "        self.neuron_coords = None\n",
    "\n",
    "        # ARCHITECTURE FIX #7: Store comprehensive metadata\n",
    "        self.metadata = {\n",
    "            \"creation_time\": datetime.now().isoformat(),\n",
    "            \"training_history\": [],\n",
    "            \"total_epochs\": 0,\n",
    "            \"total_samples_seen\": 0,\n",
    "            \"config\": config.to_dict(),\n",
    "        }\n",
    "\n",
    "        # Training state for incremental training (ARCHITECTURE FIX #4)\n",
    "        self.training_state = {\n",
    "            \"epoch\": 0,\n",
    "            \"best_error\": float(\"inf\"),\n",
    "            \"no_improvement_count\": 0,\n",
    "            \"prev_update\": None,\n",
    "        }\n",
    "\n",
    "        # Callbacks list (ARCHITECTURE FIX #9)\n",
    "        self.callbacks: List[Callback] = []\n",
    "\n",
    "        # Control flag for early stopping\n",
    "        self.stop_training = False\n",
    "\n",
    "        # Distance calculator based on metric (ARCHITECTURE FIX #5)\n",
    "        self.distance_func = self._get_distance_function()\n",
    "\n",
    "        # Initialize weights and structure\n",
    "        self._initialize_structure()\n",
    "\n",
    "    def _get_distance_function(self) -> Callable:\n",
    "        \"\"\"ARCHITECTURE FIX #5: Get appropriate distance function\"\"\"\n",
    "        metric_map = {\n",
    "            DistanceMetric.EUCLIDEAN: DistanceCalculator.euclidean,\n",
    "            DistanceMetric.MANHATTAN: DistanceCalculator.manhattan,\n",
    "            DistanceMetric.COSINE: DistanceCalculator.cosine,\n",
    "            DistanceMetric.CHEBYSHEV: DistanceCalculator.chebyshev,\n",
    "        }\n",
    "        return metric_map[self.config.distance_metric]\n",
    "\n",
    "    def _create_tree(self):\n",
    "        \"\"\"FIX: Create KDTree or BallTree with appropriate metric.\n",
    "        For COSINE we don't build a tree (BallTree doesn't accept 'cosine'),\n",
    "        instead we keep a normalized weights cache for brute-force queries.\n",
    "        \"\"\"\n",
    "        # Clear any previous normalized cache\n",
    "        self._normalized_weights = None\n",
    "\n",
    "        if self.config.distance_metric == DistanceMetric.EUCLIDEAN:\n",
    "            self.tree = KDTree(self.weights_flat)\n",
    "        elif self.config.distance_metric == DistanceMetric.MANHATTAN:\n",
    "            self.tree = BallTree(self.weights_flat, metric=\"manhattan\")\n",
    "        elif self.config.distance_metric == DistanceMetric.CHEBYSHEV:\n",
    "            self.tree = BallTree(self.weights_flat, metric=\"chebyshev\")\n",
    "        elif self.config.distance_metric == DistanceMetric.COSINE:\n",
    "            # BallTree/KDTree don't support 'cosine' reliably -> use brute-force on normalized vectors\n",
    "            # Store normalized weights for fast dot-product based cosine distance calculations\n",
    "            norms = np.linalg.norm(self.weights_flat, axis=1, keepdims=True) + 1e-8\n",
    "            self._normalized_weights = (self.weights_flat / norms).astype(\n",
    "                self.config.dtype\n",
    "            )\n",
    "            # keep tree = None to signal brute-force path in _query_tree\n",
    "            self.tree = None\n",
    "        else:\n",
    "            # Fallback\n",
    "            self.tree = KDTree(self.weights_flat)\n",
    "\n",
    "    def _query_tree(self, data: np.ndarray, k: int = 1):\n",
    "        \"\"\"Query tree with appropriate preprocessing for cosine distance.\n",
    "        Returns (distances, indices) shapes: (n_samples, k)\n",
    "        \"\"\"\n",
    "        if self.config.distance_metric == DistanceMetric.COSINE:\n",
    "            # Use brute-force cosine distance on normalized vectors\n",
    "            # Normalize data\n",
    "            data = np.asarray(data, dtype=self.config.dtype)\n",
    "            norms = np.linalg.norm(data, axis=1, keepdims=True) + 1e-8\n",
    "            normalized_data = data / norms\n",
    "\n",
    "            # Ensure normalized_weights is available (created in _create_tree)\n",
    "            if getattr(self, \"_normalized_weights\", None) is None:\n",
    "                # If weights changed and tree wasn't rebuilt, compute normalized weights now\n",
    "                self._normalized_weights = self.weights_flat / (\n",
    "                    np.linalg.norm(self.weights_flat, axis=1, keepdims=True) + 1e-8\n",
    "                )\n",
    "\n",
    "            # Dot product -> cosine similarity; distance = 1 - similarity\n",
    "            # shape (n_samples, n_neurons)\n",
    "            sim = normalized_data @ self._normalized_weights.T\n",
    "            dists = 1.0 - sim\n",
    "\n",
    "            # For k == 1 return min; else return top-k\n",
    "            if k == 1:\n",
    "                idx = np.argmin(dists, axis=1)\n",
    "                min_dists = dists[np.arange(dists.shape[0]), idx]\n",
    "                return min_dists.reshape(-1, 1), idx.reshape(-1, 1)\n",
    "            else:\n",
    "                # Partial selection for speed, then sort\n",
    "                idx_part = np.argpartition(dists, kth=k - 1, axis=1)[\n",
    "                    :, :k\n",
    "                ]  # (n_samples, k)\n",
    "                # Now sort those k per-row by actual distance\n",
    "                row_idx = np.arange(dists.shape[0])[:, None]\n",
    "                sorted_order = np.argsort(dists[row_idx, idx_part], axis=1)\n",
    "                idx_sorted = idx_part[row_idx, sorted_order]\n",
    "                dists_sorted = dists[row_idx, idx_sorted]\n",
    "                return dists_sorted, idx_sorted\n",
    "        else:\n",
    "            # Use tree-based query (KDTree / BallTree)\n",
    "            if self.tree is None:\n",
    "                # safety: (re)create tree if missing\n",
    "                self._create_tree()\n",
    "            # sklearn tree.query returns (distances, indices)\n",
    "            return self.tree.query(data, k=k)\n",
    "\n",
    "    def _initialize_structure(self):\n",
    "        \"\"\"Initialize neuron coordinates and weights\"\"\"\n",
    "        # OPTIMIZATION: Create neuron coordinates instead of 4D distance matrix\n",
    "        self.neuron_coords = self._create_neuron_coordinates()\n",
    "\n",
    "        # Initialize weights if not already present (for new training)\n",
    "        if self.weights_flat is None:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _create_neuron_coordinates(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        OPTIMIZATION: Create 2D coordinates for all neurons\n",
    "        Issue #15: Support different topologies\n",
    "        \"\"\"\n",
    "        if self.config.topology == Topology.HEXAGONAL:\n",
    "            coords = []\n",
    "            for i in range(self.config.width):\n",
    "                for j in range(self.config.height):\n",
    "                    x = i\n",
    "                    y = j + (0.5 if i % 2 else 0)\n",
    "                    coords.append([x, y])\n",
    "            return np.array(coords, dtype=self.config.dtype)\n",
    "        else:\n",
    "            coords = np.array(\n",
    "                [\n",
    "                    [i, j]\n",
    "                    for i in range(self.config.width)\n",
    "                    for j in range(self.config.height)\n",
    "                ],\n",
    "                dtype=self.config.dtype,\n",
    "            )\n",
    "            return coords\n",
    "\n",
    "    def _initialize_weights(self, data: Optional[np.ndarray] = None):\n",
    "        \"\"\"\n",
    "        ARCHITECTURE FIX #6: Multiple initialization strategies\n",
    "        \"\"\"\n",
    "        if self.config.init_strategy == InitStrategy.RANDOM:\n",
    "            # Original random initialization\n",
    "            self.weights_flat = np.random.random(\n",
    "                (self.n_neurons, self.config.n_features)\n",
    "            ).astype(self.config.dtype)\n",
    "\n",
    "        elif self.config.init_strategy == InitStrategy.PCA and data is not None:\n",
    "            # FIX: Handle cases with fewer dimensions\n",
    "            from sklearn.decomposition import PCA\n",
    "\n",
    "            n_components = min(2, data.shape[1], self.config.n_features)\n",
    "\n",
    "            if n_components < 2:\n",
    "                # Handle 1D PCA case\n",
    "                pca = PCA(n_components=1)\n",
    "                pca.fit(data)\n",
    "\n",
    "                # Create 1D gradient along principal component\n",
    "                x_range = np.linspace(-3, 3, self.n_neurons)\n",
    "                points = x_range.reshape(-1, 1)\n",
    "                self.weights_flat = pca.inverse_transform(points).astype(\n",
    "                    self.config.dtype\n",
    "                )\n",
    "            else:\n",
    "                # Original 2D PCA logic\n",
    "                pca = PCA(n_components=2)\n",
    "                pca.fit(data)\n",
    "\n",
    "                # Create grid along principal components\n",
    "                x_range = np.linspace(-3, 3, self.config.width)\n",
    "                y_range = np.linspace(-3, 3, self.config.height)\n",
    "\n",
    "                self.weights_flat = np.zeros(\n",
    "                    (self.n_neurons, self.config.n_features), dtype=self.config.dtype\n",
    "                )\n",
    "\n",
    "                idx = 0\n",
    "                for i in range(self.config.width):\n",
    "                    for j in range(self.config.height):\n",
    "                        point = np.array([x_range[i], y_range[j]])\n",
    "                        self.weights_flat[idx] = pca.inverse_transform(\n",
    "                            point.reshape(1, -1)\n",
    "                        )\n",
    "                        idx += 1\n",
    "\n",
    "            # Normalize to data range\n",
    "            self.weights_flat = np.clip(self.weights_flat, 0, 1)\n",
    "\n",
    "        elif self.config.init_strategy == InitStrategy.SAMPLE and data is not None:\n",
    "            # Initialize with random samples from data\n",
    "            indices = np.random.choice(len(data), self.n_neurons, replace=True)\n",
    "            self.weights_flat = data[indices].copy().astype(self.config.dtype)\n",
    "\n",
    "        elif self.config.init_strategy == InitStrategy.LINEAR:\n",
    "            # FIX: Handle arbitrary number of features\n",
    "            self.weights_flat = np.zeros(\n",
    "                (self.n_neurons, self.config.n_features), dtype=self.config.dtype\n",
    "            )\n",
    "            for idx in range(self.n_neurons):\n",
    "                x = idx // self.config.height\n",
    "                y = idx % self.config.height\n",
    "\n",
    "                values = []\n",
    "                for f in range(self.config.n_features):\n",
    "                    if f == 0:\n",
    "                        values.append(x / max(self.config.width, 1))\n",
    "                    elif f == 1:\n",
    "                        values.append(y / max(self.config.height, 1))\n",
    "                    else:\n",
    "                        # Create additional gradients for extra features\n",
    "                        values.append(\n",
    "                            (x + y + f)\n",
    "                            / max(self.config.width + self.config.height + f, 1)\n",
    "                        )\n",
    "\n",
    "                self.weights_flat[idx] = values\n",
    "        else:\n",
    "            # Fallback to random\n",
    "            self.weights_flat = np.random.random(\n",
    "                (self.n_neurons, self.config.n_features)\n",
    "            ).astype(self.config.dtype)\n",
    "\n",
    "        # Issue #23: Ensure weights are within bounds\n",
    "        self.weights_flat = np.clip(\n",
    "            self.weights_flat,\n",
    "            self.config.weight_bounds[0],\n",
    "            self.config.weight_bounds[1],\n",
    "        )\n",
    "\n",
    "    def _compute_neuron_distances(self, bmu_coord: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        OPTIMIZATION: Compute distances from BMU to all neurons\n",
    "        Issue #15: Handle different topologies\n",
    "        \"\"\"\n",
    "        if self.config.topology == Topology.TOROIDAL:\n",
    "            # Toroidal wrap-around distance\n",
    "            dx = np.abs(self.neuron_coords[:, 0] - bmu_coord[0])\n",
    "            dy = np.abs(self.neuron_coords[:, 1] - bmu_coord[1])\n",
    "            dx = np.minimum(dx, self.config.width - dx)\n",
    "            dy = np.minimum(dy, self.config.height - dy)\n",
    "            distances = np.sqrt(dx**2 + dy**2)\n",
    "        else:\n",
    "            # Standard distance (works for rectangular and hexagonal)\n",
    "            distances = np.linalg.norm(self.neuron_coords - bmu_coord, axis=1)\n",
    "\n",
    "        # FIX: Ensure consistent dtype\n",
    "        return distances.astype(self.config.dtype)\n",
    "\n",
    "    def _get_neighborhood_mask(\n",
    "        self, bmu_idx: int, sigma: float\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        OPTIMIZATION: Get mask of neurons within neighborhood radius\n",
    "        Issues #13, #14: Cutoff radius for efficiency\n",
    "        \"\"\"\n",
    "        cutoff_radius = self.config.cutoff_factor * sigma\n",
    "        bmu_coord = self.neuron_coords[bmu_idx]\n",
    "\n",
    "        distances = self._compute_neuron_distances(bmu_coord)\n",
    "        mask = distances <= cutoff_radius\n",
    "\n",
    "        return mask, distances[mask]\n",
    "\n",
    "    def _calculate_neighborhood(\n",
    "        self, distances: np.ndarray, sigma: float\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Issues #13, #14, #22: Gaussian with cutoff and underflow protection\n",
    "        \"\"\"\n",
    "        exponent = -(distances**2) / (2 * (sigma**2))\n",
    "        exponent = np.maximum(exponent, -50)  # Issue #22: Prevent underflow\n",
    "        return np.exp(exponent).astype(self.config.dtype)\n",
    "\n",
    "    def _get_decay_value(\n",
    "        self, t: int, t_max: int, initial: float, final: float, schedule: DecaySchedule\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Issues #18, #19, #20: Flexible decay schedules with warm-up\n",
    "        \"\"\"\n",
    "        if t < self.config.warmup_steps:\n",
    "            return initial\n",
    "\n",
    "        effective_t = t - self.config.warmup_steps\n",
    "        effective_max = t_max - self.config.warmup_steps\n",
    "\n",
    "        if effective_max <= 0:\n",
    "            return final\n",
    "\n",
    "        if schedule == DecaySchedule.LINEAR:\n",
    "            return initial - (initial - final) * (effective_t / effective_max)\n",
    "        elif schedule == DecaySchedule.INVERSE:\n",
    "            return initial / (1 + effective_t / effective_max)\n",
    "        elif schedule == DecaySchedule.COSINE:\n",
    "            return (\n",
    "                final\n",
    "                + (initial - final)\n",
    "                * (1 + np.cos(np.pi * effective_t / effective_max))\n",
    "                / 2\n",
    "            )\n",
    "        elif schedule == DecaySchedule.STEP:\n",
    "            drops = effective_t // 100\n",
    "            return initial * (0.5**drops)\n",
    "        else:  # EXPONENTIAL\n",
    "            if initial > 0 and final > 0:\n",
    "                decay_rate = -np.log(final / initial) / effective_max\n",
    "                return initial * np.exp(-decay_rate * effective_t)\n",
    "            return final\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        data: np.ndarray,\n",
    "        n_iterations: Optional[int] = None,\n",
    "        callbacks: Optional[List[Callback]] = None,\n",
    "    ) -> \"SOM\":\n",
    "        \"\"\"\n",
    "        Train the SOM on data\n",
    "\n",
    "        ARCHITECTURE FIX #4: Supports incremental training\n",
    "        ARCHITECTURE FIX #9: Supports callbacks\n",
    "\n",
    "        Args:\n",
    "            data: Input data of shape (n_samples, n_features)\n",
    "            n_iterations: Number of iterations (uses config if None)\n",
    "            callbacks: List of callback objects\n",
    "\n",
    "        Returns:\n",
    "            self for method chaining\n",
    "        \"\"\"\n",
    "        # FIX: Validate input data for NaN and infinite values\n",
    "        if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
    "            raise ValueError(\"Input data contains NaN or infinite values\")\n",
    "\n",
    "        # Validate input shape (Issue #9)\n",
    "        if data.shape[1] != self.config.n_features:\n",
    "            raise ValueError(\n",
    "                f\"Expected {self.config.n_features} features, got {data.shape[1]}\"\n",
    "            )\n",
    "\n",
    "        # Issue #8: Normalize data\n",
    "        data_normalized = self._normalize_data(data)\n",
    "\n",
    "        # ARCHITECTURE FIX #6: Initialize weights with data if needed\n",
    "        if self.weights_flat is None or (\n",
    "            self.config.init_strategy in [InitStrategy.PCA, InitStrategy.SAMPLE]\n",
    "            and self.training_state[\"epoch\"] == 0\n",
    "        ):\n",
    "            self._initialize_weights(data_normalized)\n",
    "\n",
    "        # Setup iterations\n",
    "        if n_iterations is None:\n",
    "            n_iterations = self.config.n_iterations\n",
    "\n",
    "        # Setup callbacks (ARCHITECTURE FIX #9)\n",
    "        self.callbacks = callbacks or []\n",
    "        if self.config.checkpoint_interval:\n",
    "            self.callbacks.append(\n",
    "                CheckpointCallback(\n",
    "                    self.config.checkpoint_dir, self.config.checkpoint_interval\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Call training begin callbacks\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_training_begin(self)\n",
    "\n",
    "        # Main training loop\n",
    "        self._train_loop(data_normalized, n_iterations)\n",
    "\n",
    "        # Call training end callbacks\n",
    "        for callback in self.callbacks:\n",
    "            callback.on_training_end(self)\n",
    "\n",
    "        # Update metadata (ARCHITECTURE FIX #7)\n",
    "        self.metadata[\"total_epochs\"] += n_iterations\n",
    "        self.metadata[\"total_samples_seen\"] += len(data) * n_iterations\n",
    "        self.metadata[\"last_training\"] = datetime.now().isoformat()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _normalize_data(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Issue #8: Normalize input data\"\"\"\n",
    "        data = data.astype(self.config.dtype)\n",
    "        data_min = data.min(axis=0)\n",
    "        data_max = data.max(axis=0)\n",
    "        data_range = data_max - data_min\n",
    "        data_range[data_range == 0] = 1\n",
    "        return (data - data_min) / data_range\n",
    "\n",
    "    def _train_loop(self, data: np.ndarray, n_iterations: int):\n",
    "        \"\"\"Main training loop with all optimizations\"\"\"\n",
    "        # FIX: Initialize tree with correct metric\n",
    "        self._create_tree()\n",
    "\n",
    "        # Setup progress bar (Issue #10)\n",
    "        iterator = range(\n",
    "            self.training_state[\"epoch\"], self.training_state[\"epoch\"] + n_iterations\n",
    "        )\n",
    "        if self.verbose:\n",
    "            iterator = tqdm(iterator, desc=\"Training SOM\")\n",
    "\n",
    "        # FIX: Initialize momentum tracking consistently\n",
    "        if self.training_state[\"prev_update\"] is None:\n",
    "            self.training_state[\"prev_update\"] = np.zeros_like(self.weights_flat)\n",
    "\n",
    "        for t in iterator:\n",
    "            # Epoch begin callbacks\n",
    "            for callback in self.callbacks:\n",
    "                callback.on_epoch_begin(t, self)\n",
    "\n",
    "            # Check early stopping flag\n",
    "            if self.stop_training:\n",
    "                if self.verbose:\n",
    "                    print(f\"Training stopped at epoch {t}\")\n",
    "                break\n",
    "\n",
    "            # Get current parameters (Issues #18, #19, #20)\n",
    "            sigma = self._get_decay_value(\n",
    "                t,\n",
    "                self.config.n_iterations,\n",
    "                self.config.initial_sigma,\n",
    "                self.config.min_sigma,\n",
    "                self.config.sigma_decay,\n",
    "            )\n",
    "            alpha = self._get_decay_value(\n",
    "                t,\n",
    "                self.config.n_iterations,\n",
    "                self.config.initial_alpha,\n",
    "                self.config.min_alpha,\n",
    "                self.config.alpha_decay,\n",
    "            )\n",
    "\n",
    "            # Issue #7: Shuffle data\n",
    "            shuffled_indices = np.random.permutation(len(data))\n",
    "            shuffled_data = data[shuffled_indices]\n",
    "\n",
    "            # Process based on learning mode (Issue #21)\n",
    "            epoch_metrics = self._process_epoch(shuffled_data, sigma, alpha)\n",
    "\n",
    "            # FIX: Update tree periodically with correct metric\n",
    "            if (t + 1) % self.config.kdtree_rebuild_interval == 0:\n",
    "                self._create_tree()\n",
    "\n",
    "            # Check convergence (Issues #16, #17)\n",
    "            if (\n",
    "                self.config.early_stopping\n",
    "                and t % self.config.convergence_check_interval == 0\n",
    "            ):\n",
    "                if self._check_convergence(epoch_metrics):\n",
    "                    if self.verbose:\n",
    "                        print(f\"\\nConverged after {t+1} iterations\")\n",
    "                    break\n",
    "\n",
    "            # Update display\n",
    "            if self.verbose:\n",
    "                iterator.set_postfix(\n",
    "                    {\n",
    "                        \"QE\": f\"{epoch_metrics.get('qe', 0):.4f}\",\n",
    "                        \"σ\": f\"{sigma:.3f}\",\n",
    "                        \"α\": f\"{alpha:.4f}\",\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Store metrics (ARCHITECTURE FIX #7)\n",
    "            self.metadata[\"training_history\"].append(\n",
    "                {\"epoch\": t, \"metrics\": epoch_metrics, \"sigma\": sigma, \"alpha\": alpha}\n",
    "            )\n",
    "\n",
    "            # Epoch end callbacks\n",
    "            for callback in self.callbacks:\n",
    "                callback.on_epoch_end(t, self, epoch_metrics)\n",
    "\n",
    "            # Update training state\n",
    "            self.training_state[\"epoch\"] = t + 1\n",
    "\n",
    "    def _process_epoch(self, data: np.ndarray, sigma: float, alpha: float) -> Dict:\n",
    "        \"\"\"Process one epoch of training\"\"\"\n",
    "        metrics = {\"qe\": 0}\n",
    "\n",
    "        if self.config.learning_mode == LearningMode.BATCH:\n",
    "            metrics = self._process_batch(data, sigma, alpha)\n",
    "        elif self.config.learning_mode == LearningMode.MINI_BATCH:\n",
    "            metrics = self._process_mini_batch(data, sigma, alpha)\n",
    "        else:  # ONLINE\n",
    "            metrics = self._process_online(data, sigma, alpha)\n",
    "\n",
    "        # Issue #23: Clip weights\n",
    "        self.weights_flat = np.clip(\n",
    "            self.weights_flat,\n",
    "            self.config.weight_bounds[0],\n",
    "            self.config.weight_bounds[1],\n",
    "        )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def _process_batch(self, data: np.ndarray, sigma: float, alpha: float) -> Dict:\n",
    "        \"\"\"OPTIMIZATION: Vectorized batch processing\"\"\"\n",
    "        batch_updates = np.zeros_like(self.weights_flat)\n",
    "\n",
    "        # Find all BMUs at once\n",
    "        distances, bmu_indices = self._query_tree(data, k=1)\n",
    "        bmu_indices = bmu_indices.flatten()\n",
    "        total_error = np.sum(distances**2)\n",
    "\n",
    "        # Group by BMU for efficiency\n",
    "        unique_bmus = np.unique(bmu_indices)\n",
    "        for bmu_idx in unique_bmus:\n",
    "            sample_mask = bmu_indices == bmu_idx\n",
    "            samples_for_bmu = data[sample_mask]\n",
    "\n",
    "            # OPTIMIZATION: Windowed update\n",
    "            neighbor_mask, neighbor_distances = self._get_neighborhood_mask(\n",
    "                bmu_idx, sigma\n",
    "            )\n",
    "\n",
    "            if np.any(neighbor_mask):\n",
    "                theta = self._calculate_neighborhood(neighbor_distances, sigma)\n",
    "                affected_indices = np.where(neighbor_mask)[0]\n",
    "\n",
    "                for sample in samples_for_bmu:\n",
    "                    batch_updates[affected_indices] += theta[:, np.newaxis] * (\n",
    "                        sample - self.weights_flat[affected_indices]\n",
    "                    )\n",
    "\n",
    "        # Apply updates\n",
    "        batch_updates /= len(data)\n",
    "\n",
    "        # Issue #24: FIX - Consistent momentum application\n",
    "        if self.config.momentum > 0:\n",
    "            batch_updates = (\n",
    "                self.config.momentum * self.training_state[\"prev_update\"]\n",
    "                + (1 - self.config.momentum) * batch_updates\n",
    "            )\n",
    "            self.training_state[\"prev_update\"] = batch_updates.copy()\n",
    "\n",
    "        self.weights_flat += alpha * batch_updates\n",
    "\n",
    "        return {\"qe\": total_error / len(data)}\n",
    "\n",
    "    def _process_mini_batch(self, data: np.ndarray, sigma: float, alpha: float) -> Dict:\n",
    "        \"\"\"Process mini-batches\"\"\"\n",
    "        n_batches = (len(data) + self.config.batch_size - 1) // self.config.batch_size\n",
    "        total_error = 0\n",
    "\n",
    "        for batch_idx in range(n_batches):\n",
    "            start_idx = batch_idx * self.config.batch_size\n",
    "            end_idx = min(start_idx + self.config.batch_size, len(data))\n",
    "            batch_data = data[start_idx:end_idx]\n",
    "\n",
    "            # Process similar to batch mode\n",
    "            batch_metrics = self._process_batch(batch_data, sigma, alpha)\n",
    "            total_error += batch_metrics[\"qe\"] * len(batch_data)\n",
    "\n",
    "        return {\"qe\": total_error / len(data)}\n",
    "\n",
    "    def _process_online(self, data: np.ndarray, sigma: float, alpha: float) -> Dict:\n",
    "        \"\"\"Process samples one by one\"\"\"\n",
    "        total_error = 0\n",
    "\n",
    "        for sample in data:\n",
    "            # Find BMU\n",
    "            distance, bmu_idx = self._query_tree(sample.reshape(1, -1), k=1)\n",
    "            bmu_idx = bmu_idx[0, 0]\n",
    "            total_error += distance[0, 0] ** 2\n",
    "\n",
    "            # OPTIMIZATION: Windowed update\n",
    "            neighbor_mask, neighbor_distances = self._get_neighborhood_mask(\n",
    "                bmu_idx, sigma\n",
    "            )\n",
    "\n",
    "            if np.any(neighbor_mask):\n",
    "                theta = self._calculate_neighborhood(neighbor_distances, sigma)\n",
    "                affected_indices = np.where(neighbor_mask)[0]\n",
    "\n",
    "                update = theta[:, np.newaxis] * (\n",
    "                    sample - self.weights_flat[affected_indices]\n",
    "                )\n",
    "\n",
    "                # FIX: Consistent momentum tracking for all weights\n",
    "                if self.config.momentum > 0:\n",
    "                    # Apply momentum to affected indices\n",
    "                    prev_update = self.training_state[\"prev_update\"][affected_indices]\n",
    "                    update = (\n",
    "                        self.config.momentum * prev_update\n",
    "                        + (1 - self.config.momentum) * update\n",
    "                    )\n",
    "                    # Update momentum state for affected indices\n",
    "                    self.training_state[\"prev_update\"][affected_indices] = update\n",
    "                    # Zero out momentum for unaffected neurons (decay)\n",
    "                    unaffected_mask = ~neighbor_mask\n",
    "                    self.training_state[\"prev_update\"][\n",
    "                        unaffected_mask\n",
    "                    ] *= self.config.momentum\n",
    "\n",
    "                self.weights_flat[affected_indices] += alpha * update\n",
    "\n",
    "        return {\"qe\": total_error / len(data)}\n",
    "\n",
    "    def _check_convergence(self, metrics: Dict) -> bool:\n",
    "        \"\"\"Issues #16, #17: Check for convergence\"\"\"\n",
    "        current_error = metrics.get(\"qe\", float(\"inf\"))\n",
    "\n",
    "        if (\n",
    "            abs(self.training_state[\"best_error\"] - current_error)\n",
    "            < self.config.convergence_tolerance\n",
    "        ):\n",
    "            self.training_state[\"no_improvement_count\"] += 1\n",
    "            if self.training_state[\"no_improvement_count\"] >= self.config.patience:\n",
    "                return True\n",
    "        else:\n",
    "            self.training_state[\"no_improvement_count\"] = 0\n",
    "            if current_error < self.training_state[\"best_error\"]:\n",
    "                self.training_state[\"best_error\"] = current_error\n",
    "\n",
    "        return False\n",
    "\n",
    "    def predict(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Find BMU indices for input data\"\"\"\n",
    "        # FIX: Check for NaN/inf\n",
    "        if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
    "            raise ValueError(\"Input data contains NaN or infinite values\")\n",
    "\n",
    "        data_normalized = self._normalize_data(data)\n",
    "        _, bmu_indices = self._query_tree(data_normalized, k=1)\n",
    "        return bmu_indices.flatten()\n",
    "\n",
    "    def transform(self, data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        FIX: Transform data to 2D grid coordinates using neuron_coords directly\n",
    "        \"\"\"\n",
    "        bmu_indices = self.predict(data)\n",
    "        # Directly use the neuron coordinates instead of recalculating\n",
    "        return self.neuron_coords[bmu_indices]\n",
    "\n",
    "    def get_weights(self) -> np.ndarray:\n",
    "        \"\"\"Get weights in grid format\"\"\"\n",
    "        return self.weights_flat.reshape(\n",
    "            self.config.width, self.config.height, self.config.n_features\n",
    "        )\n",
    "\n",
    "    def quantization_error(self, data: np.ndarray) -> float:\n",
    "        \"\"\"Calculate quantization error for data\"\"\"\n",
    "        # FIX: Check for NaN/inf\n",
    "        if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
    "            raise ValueError(\"Input data contains NaN or infinite values\")\n",
    "\n",
    "        data_normalized = self._normalize_data(data)\n",
    "        distances, _ = self._query_tree(data_normalized, k=1)\n",
    "        return np.mean(distances**2)\n",
    "\n",
    "    def topographic_error(self, data: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        FIX: Calculate topographic error accounting for different topologies\n",
    "        \"\"\"\n",
    "        # FIX: Check for NaN/inf\n",
    "        if np.any(np.isnan(data)) or np.any(np.isinf(data)):\n",
    "            raise ValueError(\"Input data contains NaN or infinite values\")\n",
    "\n",
    "        data_normalized = self._normalize_data(data)\n",
    "        distances, indices = self._query_tree(data_normalized, k=2)\n",
    "\n",
    "        errors = 0\n",
    "        for idx1, idx2 in indices:\n",
    "            coord1 = self.neuron_coords[idx1]\n",
    "            coord2 = self.neuron_coords[idx2]\n",
    "\n",
    "            # Calculate distance based on topology\n",
    "            if self.config.topology == Topology.HEXAGONAL:\n",
    "                # For hexagonal, adjacent means distance <= 1\n",
    "                distance = np.linalg.norm(coord1 - coord2)\n",
    "                if distance > 1.1:  # Not adjacent (with small tolerance)\n",
    "                    errors += 1\n",
    "            elif self.config.topology == Topology.TOROIDAL:\n",
    "                # For toroidal, check wrap-around distance\n",
    "                dx = np.abs(coord1[0] - coord2[0])\n",
    "                dy = np.abs(coord1[1] - coord2[1])\n",
    "                dx = min(dx, self.config.width - dx)\n",
    "                dy = min(dy, self.config.height - dy)\n",
    "                distance = np.sqrt(dx**2 + dy**2)\n",
    "                if distance > np.sqrt(2):  # Not adjacent\n",
    "                    errors += 1\n",
    "            else:  # RECTANGULAR\n",
    "                # Check if BMU and 2nd BMU are adjacent\n",
    "                distance = np.linalg.norm(coord1 - coord2)\n",
    "                if distance > np.sqrt(2):  # Not adjacent\n",
    "                    errors += 1\n",
    "\n",
    "        return errors / len(data)\n",
    "\n",
    "    # ARCHITECTURE FIX #3: Save/Load functionality with error handling\n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save trained model to file\"\"\"\n",
    "        save_data = {\n",
    "            \"config\": self.config.to_dict(),\n",
    "            \"weights\": self.weights_flat,\n",
    "            \"metadata\": self.metadata,\n",
    "            \"training_state\": self.training_state,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            with open(filepath, \"wb\") as f:\n",
    "                pickle.dump(save_data, f)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f\"Model saved to {filepath}\")\n",
    "        except (IOError, OSError) as e:\n",
    "            raise IOError(f\"Failed to save model to {filepath}: {e}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, filepath: str) -> \"SOM\":\n",
    "        \"\"\"Load trained model from file\"\"\"\n",
    "        try:\n",
    "            with open(filepath, \"rb\") as f:\n",
    "                save_data = pickle.load(f)\n",
    "        except (IOError, OSError) as e:\n",
    "            raise IOError(f\"Failed to load model from {filepath}: {e}\")\n",
    "\n",
    "        # Reconstruct SOM\n",
    "        config = SOMConfig.from_dict(save_data[\"config\"])\n",
    "        som = cls(config, verbose=False)\n",
    "        som.weights_flat = save_data[\"weights\"]\n",
    "        som.metadata = save_data[\"metadata\"]\n",
    "        som.training_state = save_data[\"training_state\"]\n",
    "\n",
    "        # FIX: Rebuild tree with correct metric\n",
    "        som._create_tree()\n",
    "\n",
    "        return som\n",
    "\n",
    "    # ARCHITECTURE FIX #7: Get comprehensive info\n",
    "    def get_info(self) -> Dict:\n",
    "        \"\"\"Get comprehensive information about the SOM\"\"\"\n",
    "        return {\n",
    "            \"config\": self.config.to_dict(),\n",
    "            \"metadata\": self.metadata,\n",
    "            \"shape\": (self.config.width, self.config.height),\n",
    "            \"n_neurons\": self.n_neurons,\n",
    "            \"n_features\": self.config.n_features,\n",
    "            \"total_epochs\": self.metadata[\"total_epochs\"],\n",
    "            \"total_samples\": self.metadata[\"total_samples_seen\"],\n",
    "        }\n",
    "\n",
    "\n",
    "# Convenience function for backward compatibility\n",
    "def train(input_data, n_max_iterations, width, height, **kwargs):\n",
    "    \"\"\"\n",
    "    Backward compatible training function\n",
    "\n",
    "    All previous 23 issues fixed plus 9 architecture improvements\n",
    "    \"\"\"\n",
    "    # FIX: Check for NaN/inf in input data\n",
    "    if np.any(np.isnan(input_data)) or np.any(np.isinf(input_data)):\n",
    "        raise ValueError(\"Input data contains NaN or infinite values\")\n",
    "\n",
    "    # Create config from parameters\n",
    "    config = SOMConfig(\n",
    "        width=width,\n",
    "        height=height,\n",
    "        n_features=input_data.shape[1],\n",
    "        n_iterations=n_max_iterations,\n",
    "        **{k: v for k, v in kwargs.items() if hasattr(SOMConfig, k)},\n",
    "    )\n",
    "\n",
    "    # Create and train SOM\n",
    "    som = SOM(config, verbose=kwargs.get(\"show_progress\", True))\n",
    "    som.fit(input_data)\n",
    "\n",
    "    # Return weights in old format\n",
    "    if kwargs.get(\"track_metrics\", False):\n",
    "        history = som.metadata[\"training_history\"]\n",
    "        errors = [h[\"metrics\"][\"qe\"] for h in history]\n",
    "        return som.get_weights(), errors\n",
    "    else:\n",
    "        return som.get_weights()\n",
    "\n",
    "\n",
    "def plot_quantization_error(som, show_plot=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot quantization error over training epochs\n",
    "\n",
    "    Args:\n",
    "        som: Trained SOM object\n",
    "        show_plot: Whether to display the plot\n",
    "        save_path: Path to save the plot image (optional)\n",
    "    \"\"\"\n",
    "    if not hasattr(som, \"metadata\") or \"training_history\" not in som.metadata:\n",
    "        print(\"No training history available for plotting\")\n",
    "        return\n",
    "\n",
    "    history = som.metadata[\"training_history\"]\n",
    "    if not history:\n",
    "        print(\"No training history data available\")\n",
    "        return\n",
    "\n",
    "    epochs = [h[\"epoch\"] for h in history]\n",
    "    qe_values = [h[\"metrics\"][\"qe\"] for h in history]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, qe_values, \"b-\", linewidth=2, label=\"Quantization Error\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Quantization Error\")\n",
    "    plt.title(\"Quantization Error vs Training Epochs\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"QE plot saved to {save_path}\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def visualize_som_weights(som, show_plot=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize SOM weights as an image\n",
    "\n",
    "    Args:\n",
    "        som: Trained SOM object\n",
    "        show_plot: Whether to display the visualization\n",
    "        save_path: Path to save the visualization (optional)\n",
    "    \"\"\"\n",
    "    weights = som.get_weights()\n",
    "\n",
    "    # Handle different numbers of features\n",
    "    if weights.shape[2] == 1:\n",
    "        # Single feature - show as grayscale\n",
    "        img = weights[:, :, 0]\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img, cmap=\"viridis\", interpolation=\"nearest\")\n",
    "        plt.colorbar(label=\"Weight Value\")\n",
    "        plt.title(\"SOM Weight Visualization (Single Feature)\")\n",
    "    elif weights.shape[2] == 2:\n",
    "        # Two features - show as 2D color map\n",
    "        img = np.zeros((weights.shape[0], weights.shape[1], 3))\n",
    "        img[:, :, 0] = weights[:, :, 0]  # Red channel\n",
    "        img[:, :, 1] = weights[:, :, 1]  # Green channel\n",
    "        img[:, :, 2] = 0.5  # Blue channel (constant)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img, interpolation=\"nearest\")\n",
    "        plt.title(\"SOM Weight Visualization (2 Features as RG)\")\n",
    "    elif weights.shape[2] >= 3:\n",
    "        # Three or more features - show first 3 as RGB\n",
    "        img = weights[:, :, :3]\n",
    "        # Normalize to [0, 1] range\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(img, interpolation=\"nearest\")\n",
    "        plt.title(\"SOM Weight Visualization (First 3 Features as RGB)\")\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"SOM visualization saved to {save_path}\")\n",
    "\n",
    "    if show_plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Example 1: Basic usage with new architecture and visualization\n",
    "    print(\"Example 1: Basic SOM with visualization\")\n",
    "    data = np.random.random((100, 3)).astype(np.float32)\n",
    "\n",
    "    # ARCHITECTURE FIX #2: Configuration management\n",
    "    config = SOMConfig(\n",
    "        width=20,\n",
    "        height=20,\n",
    "        n_features=3,\n",
    "        n_iterations=200,\n",
    "        init_strategy=InitStrategy.PCA,  # ARCHITECTURE FIX #6\n",
    "        distance_metric=DistanceMetric.EUCLIDEAN,  # ARCHITECTURE FIX #5\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    # ARCHITECTURE FIX #1: Class-based design\n",
    "    som = SOM(config)\n",
    "\n",
    "    # Train the SOM\n",
    "    som.fit(data)\n",
    "\n",
    "    # Plot quantization error over epochs\n",
    "    print(\"\\nPlotting quantization error...\")\n",
    "    plot_quantization_error(som, show_plot=False, save_path=\"qe_plot.png\")\n",
    "\n",
    "    # Visualize the SOM weights\n",
    "    print(\"Visualizing SOM weights...\")\n",
    "    visualize_som_weights(som, show_plot=False, save_path=\"som_weights.png\")\n",
    "\n",
    "    # ARCHITECTURE FIX #3: Save model\n",
    "    som.save(\"trained_som.pkl\")\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "    # ARCHITECTURE FIX #7: Get comprehensive info\n",
    "    info = som.get_info()\n",
    "    print(f\"Total epochs trained: {info['total_epochs']}\")\n",
    "    print(f\"Total samples seen: {info['total_samples']}\")\n",
    "\n",
    "    # Example 2: Load and continue training (ARCHITECTURE FIX #4)\n",
    "    print(\"\\nExample 2: Incremental training\")\n",
    "\n",
    "    # Load saved model\n",
    "    som_loaded = SOM.load(\"trained_som.pkl\")\n",
    "    print(\"Model loaded!\")\n",
    "\n",
    "    # Continue training with new data\n",
    "    new_data = np.random.random((50, 3)).astype(np.float32)\n",
    "    som_loaded.fit(new_data, n_iterations=100)\n",
    "    print(\n",
    "        f\"Additional training complete. Total epochs: {som_loaded.metadata['total_epochs']}\"\n",
    "    )\n",
    "\n",
    "    # Plot updated quantization error\n",
    "    plot_quantization_error(\n",
    "        som_loaded, show_plot=False, save_path=\"qe_plot_continued.png\"\n",
    "    )\n",
    "\n",
    "    # Example 3: Different initialization strategies with visualization\n",
    "    print(\"\\nExample 3: Comparing initialization strategies\")\n",
    "\n",
    "    strategies = [InitStrategy.RANDOM, InitStrategy.PCA, InitStrategy.LINEAR]\n",
    "\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        config = SOMConfig(\n",
    "            width=10,\n",
    "            height=10,\n",
    "            n_features=3,\n",
    "            n_iterations=100,\n",
    "            init_strategy=strategy,\n",
    "            seed=42,\n",
    "        )\n",
    "        som = SOM(config, verbose=False)\n",
    "        som.fit(data)\n",
    "        qe = som.quantization_error(data)\n",
    "        print(f\"{strategy.value}: QE = {qe:.4f}\")\n",
    "\n",
    "        # Save visualization for each strategy\n",
    "        visualize_som_weights(\n",
    "            som, show_plot=False, save_path=f\"som_{strategy.value.lower()}.png\"\n",
    "        )\n",
    "\n",
    "    # Example 4: Different distance metrics\n",
    "    print(\"\\nExample 4: Comparing distance metrics\")\n",
    "\n",
    "    metrics = [\n",
    "        DistanceMetric.EUCLIDEAN,\n",
    "        DistanceMetric.MANHATTAN,\n",
    "        DistanceMetric.COSINE,\n",
    "    ]\n",
    "\n",
    "    for metric in metrics:\n",
    "        config = SOMConfig(\n",
    "            width=10,\n",
    "            height=10,\n",
    "            n_features=3,\n",
    "            n_iterations=100,\n",
    "            distance_metric=metric,\n",
    "            seed=42,\n",
    "        )\n",
    "        som = SOM(config, verbose=False)\n",
    "        som.fit(data)\n",
    "        qe = som.quantization_error(data)\n",
    "        print(f\"{metric.value}: QE = {qe:.4f}\")\n",
    "\n",
    "    # Example 5: Backward compatibility\n",
    "    print(\"\\nExample 5: Backward compatible function\")\n",
    "    weights = train(data, 100, 10, 10, seed=42, show_progress=False)\n",
    "    print(f\"Weights shape: {weights.shape}\")\n",
    "\n",
    "    # Example 6: Test with different feature counts and visualizations\n",
    "    print(\"\\nExample 6: Testing with different feature counts\")\n",
    "    for n_features in [1, 2, 3, 5]:\n",
    "        test_data = np.random.random((50, n_features)).astype(np.float32)\n",
    "        config = SOMConfig(\n",
    "            width=8,\n",
    "            height=8,\n",
    "            n_features=n_features,\n",
    "            n_iterations=50,\n",
    "            init_strategy=InitStrategy.LINEAR,\n",
    "            seed=42,\n",
    "        )\n",
    "        som = SOM(config, verbose=False)\n",
    "        som.fit(test_data)\n",
    "        print(f\"n_features={n_features}: Training successful\")\n",
    "\n",
    "        # Visualize each case\n",
    "        visualize_som_weights(\n",
    "            som, show_plot=False, save_path=f\"som_{n_features}features.png\"\n",
    "        )\n",
    "\n",
    "    # Save final result from original weights\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    if weights.shape[2] >= 3:\n",
    "        img = weights[:, :, :3]\n",
    "        img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
    "        plt.imshow(img, interpolation=\"nearest\")\n",
    "    else:\n",
    "        plt.imshow(weights[:, :, 0], cmap=\"viridis\", interpolation=\"nearest\")\n",
    "        plt.colorbar()\n",
    "    plt.title(\"Final SOM Visualization\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.savefig(\"som_final.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"\\nFinal visualization saved as som_final.png\")\n",
    "\n",
    "    print(\"\\nAll visualizations have been saved as PNG files:\")\n",
    "    print(\"- qe_plot.png: Quantization error over epochs\")\n",
    "    print(\"- som_weights.png: Main SOM weight visualization\")\n",
    "    print(\"- qe_plot_continued.png: QE after continued training\")\n",
    "    print(\"- som_random.png, som_pca.png, som_linear.png: Different init strategies\")\n",
    "    print(\"- som_1features.png through som_5features.png: Different feature counts\")\n",
    "    print(\"- som_final.png: Final result\")\n",
    "\n",
    "    # Clean up\n",
    "    import shutil\n",
    "\n",
    "    if os.path.exists(\"checkpoints\"):\n",
    "        shutil.rmtree(\"checkpoints\")\n",
    "    if os.path.exists(\"trained_som.pkl\"):\n",
    "        os.remove(\"trained_som.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kohonen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
